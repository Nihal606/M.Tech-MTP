{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cpu\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyvista'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNumIntg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# import rff\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyvista\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpv\u001b[39;00m\n\u001b[0;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2022\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m'''This line sets the random seed for PyTorch's random number generators. Setting a seed ensures that the results\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m are reproducible. That is, if you run the same code with the same seed, you'll get the same results every time.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m   Here, the seed is set to 2022.'''\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyvista'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "import h5py\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import matplotlib as mpl\n",
    "import numpy.random as npr\n",
    "import scipy.integrate as sp\n",
    "from pyevtk.hl import gridToVTK\n",
    "import pandas as pd \n",
    "import numpy.linalg as la\n",
    "from torch.multiprocessing import Process, Pool\n",
    "from NumIntg import *\n",
    "# import rff\n",
    "import pyvista as pv\n",
    "torch.manual_seed(2022)\n",
    "'''This line sets the random seed for PyTorch's random number generators. Setting a seed ensures that the results\n",
    " are reproducible. That is, if you run the same code with the same seed, you'll get the same results every time.\n",
    "   Here, the seed is set to 2022.'''\n",
    "mpl.rcParams['figure.dpi'] = 350\n",
    "'''This line sets the resolution of the figures created using Matplotlib (often abbreviated as mpl). \n",
    "The 'figure.dpi' parameter specifies the dots per inch\n",
    " (DPI) for the figures, which affects the quality and size of the output images.\n",
    "   Setting this to 350 means the figures will be high-resolution.'''\n",
    "\n",
    "torch.cuda.is_available = lambda : False\n",
    "'''This line overrides the torch.cuda.is_available function with a lambda function that always returns False. This \n",
    "is typically used to simulate a scenario where CUDA (GPU support) is not available, regardless of whether \n",
    "it's actually available on the system.'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "'''This line sets the device variable to 'cuda' if CUDA is available; otherwise, \n",
    "it sets it to 'cpu'. torch.device is used to specify the device on which tensors\n",
    " will be allocated and computations will be performed.'''\n",
    "\n",
    "\n",
    "dev = torch.device('cpu')\n",
    "'''This line sets the dev variable to 'cpu'. It's a default setting that assumes the computations will run on the CPU.'''\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available, running on GPU\")\n",
    "    dev = torch.device('cuda')\n",
    "    device_string = 'cuda'\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device_string = 'cpu'\n",
    "    print(\"CUDA not available, running on CPU\")\n",
    "'''The code is used to configure the environment for PyTorch computations based on the availability of CUDA (GPU support).\n",
    " It sets up reproducibility, high-resolution figure output, and determines whether to use CPU or \n",
    "GPU for computations. The overriding of torch.cuda.is_available with a lambda function is used to\n",
    " simulate a scenario where CUDA is unavailable.'''\n",
    "\n",
    "\n",
    "def setup_domain():\n",
    "    x_dom = 0, Length, Nx           #x_dom is a tuple\n",
    "    y_dom = 0, Width,  Ny           #y_dom is a tuple\n",
    "    z_dom = 0, Depth,  Nz           #z_dom is a tuple\n",
    "    # create points\n",
    "    lin_x = np.linspace(x_dom[0], x_dom[1], x_dom[2])      # 1D array containing Nx points including 0 and 4(length)\n",
    "    lin_y = np.linspace(y_dom[0], y_dom[1], y_dom[2])\n",
    "    lin_z = np.linspace(z_dom[0], z_dom[1], z_dom[2])\n",
    "    domEn = np.zeros((Nx * Ny * Nz, 3))                     #2D array containing of shape(3700,3)\n",
    "    c = 0\n",
    "    '''The order of iteration is chosen to match\n",
    "    the memory layout of an array, without considering a particular ordering.https://www.geeksforgeeks.org/numpy-iterating-over-array/'''\n",
    "                                                          #NumPy package contains an iterator object numpy.nditer.\n",
    "                                                            # It is an efficient multidimensional iterator \n",
    "                                                            #object using which it is possible to iterate over an array.\n",
    "    for z in np.nditer(lin_z):                              # Each element of an array is visited using Python’s standard Iterator interface.\n",
    "        for x in np.nditer(lin_x):\n",
    "            tb = y_dom[2] * c\n",
    "            te = tb + y_dom[2]\n",
    "            c += 1\n",
    "            domEn[tb:te, 0] = x\n",
    "            domEn[tb:te, 1] = lin_y\n",
    "            domEn[tb:te, 2] = z\n",
    "    print('Uniform Nodes', domEn.shape)\n",
    "    np.meshgrid(lin_x, lin_y, lin_z)\n",
    "\n",
    "    dom = domEn\n",
    "    \n",
    "    # ------------------------------------ BOUNDARY ----------------------------------------\n",
    "    # Left boundary condition (Dirichlet BC)\n",
    "    bcl_u_pts_idx = np.where(dom[:, 0] == 0)            #gives a tuple of row index of points having x-coordinate zero\n",
    "    bcl_u_pts = dom[bcl_u_pts_idx, :][0]                #gives the coordinates of points having x-cordinate zero\n",
    "    # Right boundary condition (Neumann BC)\n",
    "    bcr_t_pts_idx = np.where(dom[:, 0] == Length)\n",
    "    bcr_t_pts = dom[bcr_t_pts_idx, :][0]\n",
    "    bcr_t_pts_idx_uniform = np.where(domEn[:, 0] == Length)\n",
    "    bcr_t_pts_uniform = domEn[bcr_t_pts_idx_uniform, :][0]\n",
    "    top_idx = np.where((dom[:, 1]==Width) & (dom[:, 0]>0) & (dom[:, 0]<Length))\n",
    "    top_pts = dom[top_idx, :][0]\n",
    "    bottom_idx = np.where((dom[:, 1] == 0) & (dom[:, 0]>0) & (dom[:, 0]<Length))\n",
    "    bottom_pts = dom[bottom_idx, :][0]\n",
    "    front_idx = np.where((dom[:, 2] == Depth) & (dom[:, 0]>0) & (dom[:, 0]<Length))\n",
    "    front_pts = dom[front_idx, :][0]\n",
    "    back_idx = np.where((dom[:, 2] == 0) & (dom[:, 0]>0) & (dom[:, 0]<Length))\n",
    "    back_pts = dom[back_idx, :][0]\n",
    "    allnodes = np.arange(len(dom))\n",
    "    inn_nodes_indx = np.setdiff1d(allnodes,bcr_t_pts_idx)\n",
    "    inn_nodes_indx = np.setdiff1d(inn_nodes_indx,bcl_u_pts_idx)\n",
    "    inn_nodes_indx = np.setdiff1d(inn_nodes_indx,top_idx)\n",
    "    inn_nodes_indx = np.setdiff1d(inn_nodes_indx,bottom_idx)\n",
    "    inn_nodes_indx = np.setdiff1d(inn_nodes_indx,front_idx)\n",
    "    inn_nodes_indx = np.setdiff1d(inn_nodes_indx,back_idx)\n",
    "    inn_nodes = dom[inn_nodes_indx,:]\n",
    "\n",
    "\n",
    "    domain           = {}\n",
    "    domain['Domain'] = torch.from_numpy(dom).float()\n",
    "    domain['Energy'] = torch.from_numpy(domEn).float()\n",
    "    domain['Xint']   = torch.from_numpy(inn_nodes_indx).long()\n",
    "    domain['X1']     = torch.from_numpy(bcr_t_pts_idx[0]).long()\n",
    "    domain['X1_Uni'] = torch.from_numpy(bcr_t_pts_idx_uniform[0]).long()\n",
    "    domain['X2']     = torch.from_numpy(bcl_u_pts_idx[0]).long()\n",
    "    domain['Y1']     = torch.from_numpy(top_idx[0]).long()\n",
    "    domain['Y2']     = torch.from_numpy(bottom_idx[0]).long()\n",
    "    domain['Z1']     = torch.from_numpy(front_idx[0]).long()\n",
    "    domain['Z2']     = torch.from_numpy(back_idx[0]).long()\n",
    "    \n",
    "    return domain\n",
    "\n",
    "class S_Net(torch.nn.Module):                               #defines the network architecture\n",
    "    def __init__(self, D_in, H, D_out , act_fn):\n",
    "        super(S_Net, self).__init__()\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        # self.encoding = rff.layers.GaussianEncoding(sigma=0.05, input_size=D_in, encoded_size=H//2)\n",
    "        # self.encoding = rff.layers.PositionalEncoding(sigma=0.25, m=10)\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)             #(3,16)\n",
    "        self.linear2 = torch.nn.Linear(H, 2*H)              #(16,32)\n",
    "        self.linear3 = torch.nn.Linear(2*H, 4*H)            #(32,64)\n",
    "        self.linear4 = torch.nn.Linear(4*H, 2*H)            #(64,32)\n",
    "        self.linear5 = torch.nn.Linear(2*H, H)              #(32,16)\n",
    "\n",
    "        self.linear6 = torch.nn.Linear(H, D_out)            #(16,3)\n",
    "        \n",
    "    def forward(self, x ):\n",
    "        af_mapping = { 'tanh' : torch.tanh ,\n",
    "                        'relu' : torch.nn.ReLU() ,\n",
    "                        'rrelu' : torch.nn.RReLU() ,\n",
    "                        'sigmoid' : torch.sigmoid }\n",
    "        activation_fn = af_mapping[ self.act_fn ]  \n",
    "          \n",
    "        \n",
    "        # y = self.encoding(x)\n",
    "        y = activation_fn(self.linear1(x))\n",
    "        y = activation_fn(self.linear2(y))\n",
    "        y = activation_fn(self.linear3(y))\n",
    "        y = activation_fn(self.linear4(y))\n",
    "        y = activation_fn(self.linear5(y))\n",
    "\n",
    "        # Output\n",
    "        y = self.linear6(y)\n",
    "        return y\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "                if isinstance(m, torch.nn.Linear):\n",
    "                    torch.nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                    torch.nn.init.normal_(m.bias, mean=0, std=0.1)\n",
    "                    \n",
    "def loss_sum(tinput):\n",
    "    return torch.sum(tinput) / tinput.data.nelement()\n",
    "    \n",
    "def innerproduct(A,B):\n",
    "    Z = (A[:,0,0] * B[:,0,0] + A[:,0,1] * B[:,0,1] + A[:,0,2] * B[:,0,2] +\n",
    "         A[:,1,0] * B[:,1,0] + A[:,1,1] * B[:,1,1] + A[:,1,2] * B[:,1,2] +\n",
    "         A[:,2,0] * B[:,2,0] + A[:,2,1] * B[:,2,1] + A[:,2,2] * B[:,2,2])\n",
    "         \n",
    "    return Z\n",
    "    \n",
    "def determinant(F):\n",
    "\n",
    "    detF = (F[:,0,0] * (F[:,1,1] * F[:,2,2] - F[:,1,2] * F[:,2,1])) - (\n",
    "            F[:,0,1] * (F[:,1,0] * F[:,2,2] - F[:,1,2] * F[:,2,0])) + (\n",
    "            F[:,0,2] * (F[:,1,0] * F[:,2,1] - F[:,1,1] * F[:,2,0]))\n",
    "    \n",
    "    return detF\n",
    "    \n",
    "def inverse(F):\n",
    "    \n",
    "    detF = determinant(F)\n",
    "    F_inv = torch.empty((len(F),3,3))              \n",
    "    F_inv[:,0,0] =  (F[:,1,1] * F[:,2,2] - F[:,1,2] * F[:,2,1]) / detF\n",
    "    F_inv[:,0,1] = -(F[:,0,1] * F[:,2,2] - F[:,0,2] * F[:,2,1]) / detF\n",
    "    F_inv[:,0,2] =  (F[:,0,1] * F[:,1,2] - F[:,0,2] * F[:,1,1]) / detF\n",
    "    F_inv[:,1,0] = -(F[:,1,0] * F[:,2,2] - F[:,1,2] * F[:,2,0]) / detF\n",
    "    F_inv[:,1,1] =  (F[:,0,0] * F[:,2,2] - F[:,0,2] * F[:,2,0]) / detF\n",
    "    F_inv[:,1,2] = -(F[:,0,0] * F[:,1,2] - F[:,0,2] * F[:,1,0]) / detF\n",
    "    F_inv[:,2,0] =  (F[:,1,0] * F[:,2,1] - F[:,1,1] * F[:,2,0]) / detF\n",
    "    F_inv[:,2,1] = -(F[:,0,0] * F[:,2,1] - F[:,0,1] * F[:,2,0]) / detF\n",
    "    F_inv[:,2,2] =  (F[:,0,0] * F[:,1,1] - F[:,0,1] * F[:,1,0]) / detF\n",
    "    \n",
    "    return F_inv\n",
    "\n",
    "def trace(A):\n",
    "\n",
    "    trace_A = A[:,0,0] + A[:,1,1] + A[:,2,2]\n",
    "\n",
    "    return trace_A\n",
    "\n",
    "def displacement_gradient(u,x):\n",
    "\n",
    "    gradu = torch.empty((len(x),3,3))\n",
    "    \n",
    "    duxdxyz = grad(u[:, 0].unsqueeze(1), x, torch.ones(x.size()[0], 1, device=dev), create_graph=True, retain_graph=True)[0]\n",
    "    duydxyz = grad(u[:, 1].unsqueeze(1), x, torch.ones(x.size()[0], 1, device=dev), create_graph=True, retain_graph=True)[0]\n",
    "    duzdxyz = grad(u[:, 2].unsqueeze(1), x, torch.ones(x.size()[0], 1, device=dev), create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    du11 = duxdxyz[:, 0].unsqueeze(1); du12 = duxdxyz[:, 1].unsqueeze(1); du13 = duxdxyz[:, 2].unsqueeze(1)\n",
    "    du21 = duydxyz[:, 0].unsqueeze(1); du22 = duydxyz[:, 1].unsqueeze(1); du23 = duydxyz[:, 2].unsqueeze(1)\n",
    "    du31 = duzdxyz[:, 0].unsqueeze(1); du32 = duzdxyz[:, 1].unsqueeze(1); du33 = duzdxyz[:, 2].unsqueeze(1)\n",
    "    \n",
    "    gradu[:,0,0] = du11.squeeze(1); gradu[:,0,1] = du12.squeeze(1); gradu[:,0,2] = du13.squeeze(1)\n",
    "    gradu[:,1,0] = du21.squeeze(1); gradu[:,1,1] = du22.squeeze(1); gradu[:,1,2] = du23.squeeze(1)\n",
    "    gradu[:,2,0] = du31.squeeze(1); gradu[:,2,1] = du32.squeeze(1); gradu[:,2,2] = du33.squeeze(1)\n",
    "\n",
    "    # For diagonal case\n",
    "    # gradu[:,0,1]=0; gradu[:,0,2]=0; gradu[:,1,0]=0; gradu[:,1,2]=0; gradu[:,2,0]=0; gradu[:,2,1]=0\n",
    "\n",
    "    return gradu\n",
    "\n",
    "def deformation_gradient(u,x):\n",
    "\n",
    "    identity = torch.zeros((len(x), 3, 3)); identity[:,0,0]=1; identity[:,1,1]=1; identity[:,2,2]=1\n",
    "    gradu    = displacement_gradient(u, x)\n",
    "    F        = identity + gradu\n",
    "\n",
    "    return F\n",
    "    \n",
    "def stressNH(F):\n",
    "    # Material Properties\n",
    "    lmbdaNH   = YM * PR /(1+PR)/(1-2*PR)\n",
    "    muNH      = YM/2/(1+PR)\n",
    "    Finv     = inverse(F)\n",
    "    detF     = determinant(F)\n",
    "    stressPK = muNH * F + (lmbdaNH * torch.log(detF) - muNH).view(-1,1,1)*Finv.permute(0,2,1)\n",
    "\n",
    "    return stressPK\n",
    "\n",
    "\n",
    "def psi(u_pred, x, integrationIE, dx, dy, dz, shape):\n",
    "    mu = YM / ( 2. * ( 1. + PR ) )\n",
    "    K = YM / ( 3. * ( 1. - 2. * PR ) )\n",
    "    C10 = mu / 2.\n",
    "    D1 = 2. / K\n",
    "    # print( C10 , D1 )\n",
    "\n",
    "    F    = deformation_gradient(u_pred, x)\n",
    "    detF = determinant(F)\n",
    "    F_bar = torch.einsum( 'i,ijk->ijk' , torch.pow( detF , -1./3. ) , F )\n",
    "    B_bar    = torch.bmm( F_bar , F_bar.permute(0,2,1) )\n",
    "    I1   = trace( B_bar )\n",
    "\n",
    "    psiE = C10 * ( I1 - 3. ) + torch.pow( detF - 1 , 2. ) / D1\n",
    "\n",
    "    internal_1 = integrationIE(psiE, dx=dx, dy=dy, dz=dz, shape=[shape[0], shape[1], shape[2]])\n",
    "    \n",
    "    return internal_1\n",
    "\n",
    "\n",
    "def psi_Gauss(u, x, integrationIE, dx, dy, dz, shape):\n",
    "    mu = YM / ( 2. * ( 1. + PR ) )\n",
    "    K = YM / ( 3. * ( 1. - 2. * PR ) )\n",
    "    C10 = mu / 2.\n",
    "    D1 = 2. / K\n",
    "\n",
    "    N_element = ( shape[0] - 1 ) * ( shape[1] - 1 ) * ( shape[2] - 1 )\n",
    "    order = [ 1 ,  shape[-1] , shape[0] , shape[1] ]\n",
    "    Ux = torch.transpose(u[:, 0].reshape( order ), 2, 3)\n",
    "    Uy = torch.transpose(u[:, 1].reshape( order ), 2, 3)\n",
    "    Uz = torch.transpose(u[:, 2].reshape( order ), 2, 3)\n",
    "    U = torch.cat( (Ux,Uy,Uz) , dim=0 )\n",
    "\n",
    "    #        dim  z      y     x\n",
    "    U_N1 = U[ : , :-1 , :-1 , :-1 ]\n",
    "    U_N2 = U[ : , :-1 , :-1 , 1: ]\n",
    "    U_N3 = U[ : , 1: , :-1 , 1: ]\n",
    "    U_N4 = U[ : , 1: , :-1 , :-1 ]\n",
    "    U_N5 = U[ : , :-1 , 1: , :-1 ]\n",
    "    U_N6 = U[ : , :-1 , 1: , 1: ]\n",
    "    U_N7 = U[ : , 1: , 1: , 1: ]\n",
    "    U_N8 = U[ : , 1: , 1: , :-1 ]\n",
    "    U_N = torch.stack( [ U_N1 , U_N2 , U_N3 , U_N4 , U_N5 , U_N6 , U_N7 , U_N8 ] )#.double()\n",
    "\n",
    "    # Compute constants\n",
    "    detJ = dx*dy*dz / 8.\n",
    "    Jinv = torch.zeros([3,3]).double()\n",
    "    dxdydz = [ dx , dy , dz ]\n",
    "    for i in range(3):\n",
    "        Jinv[i,i] = 2. / dxdydz[i]\n",
    "    identity = torch.zeros((N_element, 3, 3)); identity[:,0,0]=1; identity[:,1,1]=1; identity[:,2,2]=1\n",
    "\n",
    "\n",
    "    # Go through all integration pts\n",
    "    strainEnergy_at_elem = torch.zeros( N_element )\n",
    "\n",
    "    vv = np.sqrt( 1. / 3. )\n",
    "    pt = [-vv,vv]\n",
    "    intpt = torch.tensor([[pt[0],pt[0],pt[0]],\n",
    "                          [pt[1],pt[0],pt[0]],\n",
    "                          [pt[1],pt[1],pt[0]],\n",
    "                          [pt[0],pt[1],pt[0]],\n",
    "                          [pt[0],pt[0],pt[1]],\n",
    "                          [pt[1],pt[0],pt[1]],\n",
    "                          [pt[1],pt[1],pt[1]],\n",
    "                          [pt[0],pt[1],pt[1]]])\n",
    "\n",
    "    for i in range( 8 ):\n",
    "        x_ , y_ , z_ = intpt[i,:]\n",
    "        # Shape grad in natural coords\n",
    "        B = torch.tensor([[-((y_ - 1)*(z_ - 1))/8, -((x_ - 1)*(z_ - 1))/8, -((x_ - 1)*(y_ - 1))/8],\n",
    "                    [ ((y_ - 1)*(z_ - 1))/8,  ((x_ + 1)*(z_ - 1))/8,  ((x_ + 1)*(y_ - 1))/8],\n",
    "                    [-((y_ - 1)*(z_ + 1))/8, -((x_ + 1)*(z_ + 1))/8, -((x_ + 1)*(y_ - 1))/8],\n",
    "                    [ ((y_ - 1)*(z_ + 1))/8,  ((x_ - 1)*(z_ + 1))/8,  ((x_ - 1)*(y_ - 1))/8],\n",
    "                    [ ((y_ + 1)*(z_ - 1))/8,  ((x_ - 1)*(z_ - 1))/8,  ((x_ - 1)*(y_ + 1))/8],\n",
    "                    [-((y_ + 1)*(z_ - 1))/8, -((x_ + 1)*(z_ - 1))/8, -((x_ + 1)*(y_ + 1))/8],\n",
    "                    [ ((y_ + 1)*(z_ + 1))/8,  ((x_ + 1)*(z_ + 1))/8,  ((x_ + 1)*(y_ + 1))/8],\n",
    "                    [-((y_ + 1)*(z_ + 1))/8, -((x_ - 1)*(z_ + 1))/8, -((x_ - 1)*(y_ + 1))/8]]).double()\n",
    "        \n",
    "        # Convert to physical gradient\n",
    "        B_physical = torch.matmul( B , Jinv ).double()\n",
    "        dUx = torch.einsum( 'ijkl,iq->qjkl' , U_N[:,0,:,:,:] , B_physical )\n",
    "        dUy = torch.einsum( 'ijkl,iq->qjkl' , U_N[:,1,:,:,:] , B_physical )\n",
    "        dUz = torch.einsum( 'ijkl,iq->qjkl' , U_N[:,2,:,:,:] , B_physical )\n",
    "        grad_u = torch.reshape( torch.transpose( torch.flatten( torch.cat( (dUx,dUy,dUz) , dim=0 ) ,  start_dim=1, end_dim=-1 ) , 0 , 1 ) , [N_element,3,3] )\n",
    "\n",
    "        # Def grad\n",
    "        F = grad_u + identity\n",
    "\n",
    "        detF = determinant( F )\n",
    "        F_bar = torch.einsum( 'i,ijk->ijk' , torch.pow( detF , -1./3. ) , F )\n",
    "        B_bar    = torch.bmm( F_bar , F_bar.permute(0,2,1) )\n",
    "        I1   = trace( B_bar )\n",
    "\n",
    "        psiE = C10 * ( I1 - 3. ) + torch.pow( detF - 1 , 2. ) / D1\n",
    "\n",
    "        strainEnergy_at_elem += psiE * 1. * detJ    \n",
    "    return torch.sum( strainEnergy_at_elem )\n",
    "\n",
    "\n",
    "def stressLE( e ):\n",
    "    lame1 = YM * PR / ( ( 1. + PR ) * ( 1. - 2. * PR ) )\n",
    "    mu = YM / ( 2. * ( 1. + PR ) )    \n",
    "\n",
    "    identity = torch.zeros((len(e), 3, 3)); identity[:,0,0]=1; identity[:,1,1]=1; identity[:,2,2]=1\n",
    "\n",
    "    trace_e = e[:,0,0] + e[:,1,1] + e[:,2,2]\n",
    "    return lame1 * torch.einsum( 'ijk,i->ijk' , identity , trace_e ) + 2 * mu * e\n",
    "\n",
    "\n",
    "def LE(u_pred, x, integrationIE, dx, dy, dz, shape):\n",
    "    grad_u    = displacement_gradient(u_pred, x)\n",
    "    strain = 0.5 * ( grad_u + grad_u.permute(0,2,1) )\n",
    "\n",
    "    stress = stressLE( strain )\n",
    "\n",
    "    psiE = 0.5 * torch.einsum( 'ijk,ijk->i' , stress , strain )\n",
    "\n",
    "    internal_1 = integrationIE(psiE, dx=dx, dy=dy, dz=dz, shape=[shape[0], shape[1], shape[2]])\n",
    "    \n",
    "    return internal_1\n",
    "\n",
    "\n",
    "def LE_Gauss(u, x, integrationIE, dx, dy, dz, shape):\n",
    "    N_element = ( shape[0] - 1 ) * ( shape[1] - 1 ) * ( shape[2] - 1 )\n",
    "    order = [ 1 ,  shape[-1] , shape[0] , shape[1] ]\n",
    "    Ux = torch.transpose(u[:, 0].reshape( order ), 2, 3)\n",
    "    Uy = torch.transpose(u[:, 1].reshape( order ), 2, 3)\n",
    "    Uz = torch.transpose(u[:, 2].reshape( order ), 2, 3)\n",
    "    U = torch.cat( (Ux,Uy,Uz) , dim=0 )\n",
    "\n",
    "    #        dim  z      y     x\n",
    "    U_N1 = U[ : , :-1 , :-1 , :-1 ]\n",
    "    U_N2 = U[ : , :-1 , :-1 , 1: ]\n",
    "    U_N3 = U[ : , 1: , :-1 , 1: ]\n",
    "    U_N4 = U[ : , 1: , :-1 , :-1 ]\n",
    "    U_N5 = U[ : , :-1 , 1: , :-1 ]\n",
    "    U_N6 = U[ : , :-1 , 1: , 1: ]\n",
    "    U_N7 = U[ : , 1: , 1: , 1: ]\n",
    "    U_N8 = U[ : , 1: , 1: , :-1 ]\n",
    "    U_N = torch.stack( [ U_N1 , U_N2 , U_N3 , U_N4 , U_N5 , U_N6 , U_N7 , U_N8 ] )#.double()\n",
    "\n",
    "    # Compute constants\n",
    "    detJ = dx*dy*dz / 8.\n",
    "    Jinv = torch.zeros([3,3]).double()\n",
    "    dxdydz = [ dx , dy , dz ]\n",
    "    for i in range(3):\n",
    "        Jinv[i,i] = 2. / dxdydz[i]\n",
    "\n",
    "    grad2strain = torch.zeros([6,9]).double()\n",
    "    grad2strain[0,0] = 1. # 11\n",
    "    grad2strain[1,4] = 1. # 22\n",
    "    grad2strain[2,8] = 1. # 33\n",
    "    grad2strain[3,5] = 0.5; grad2strain[3,7] = 0.5 # 23\n",
    "    grad2strain[4,2] = 0.5; grad2strain[4,6] = 0.5 # 13\n",
    "    grad2strain[5,1] = 0.5; grad2strain[5,3] = 0.5 # 12 \n",
    "\n",
    "    C_elastic = torch.zeros([6,6]).double()\n",
    "    C_elastic[0,0] = 1. - PR; C_elastic[0,1] = PR; C_elastic[0,2] = PR\n",
    "    C_elastic[1,0] = PR; C_elastic[1,1] = 1. - PR; C_elastic[1,2] = PR\n",
    "    C_elastic[2,0] = PR; C_elastic[2,1] = PR; C_elastic[2,2] = 1. - PR\n",
    "    C_elastic[3,3] = 1. - 2. * PR;\n",
    "    C_elastic[4,4] = 1. - 2. * PR;\n",
    "    C_elastic[5,5] = 1. - 2. * PR;\n",
    "    C_elastic *= ( YM / ( ( 1. + PR ) * ( 1. - 2. * PR ) ) )\n",
    "\n",
    "    # Go through all integration pts\n",
    "    strainEnergy_at_elem = torch.zeros( [ shape[-1] -1 , shape[1] -1 , shape[0] -1 ] )\n",
    "\n",
    "    vv = np.sqrt( 1. / 3. )\n",
    "    pt = [-vv,vv]\n",
    "    intpt = torch.tensor([[pt[0],pt[0],pt[0]],\n",
    "                          [pt[1],pt[0],pt[0]],\n",
    "                          [pt[1],pt[1],pt[0]],\n",
    "                          [pt[0],pt[1],pt[0]],\n",
    "                          [pt[0],pt[0],pt[1]],\n",
    "                          [pt[1],pt[0],pt[1]],\n",
    "                          [pt[1],pt[1],pt[1]],\n",
    "                          [pt[0],pt[1],pt[1]]])\n",
    "\n",
    "    for i in range( 8 ):\n",
    "        x_ , y_ , z_ = intpt[i,:]\n",
    "        # Shape grad in natural coords\n",
    "        B = torch.tensor([[-((y_ - 1)*(z_ - 1))/8, -((x_ - 1)*(z_ - 1))/8, -((x_ - 1)*(y_ - 1))/8],\n",
    "                    [ ((y_ - 1)*(z_ - 1))/8,  ((x_ + 1)*(z_ - 1))/8,  ((x_ + 1)*(y_ - 1))/8],\n",
    "                    [-((y_ - 1)*(z_ + 1))/8, -((x_ + 1)*(z_ + 1))/8, -((x_ + 1)*(y_ - 1))/8],\n",
    "                    [ ((y_ - 1)*(z_ + 1))/8,  ((x_ - 1)*(z_ + 1))/8,  ((x_ - 1)*(y_ - 1))/8],\n",
    "                    [ ((y_ + 1)*(z_ - 1))/8,  ((x_ - 1)*(z_ - 1))/8,  ((x_ - 1)*(y_ + 1))/8],\n",
    "                    [-((y_ + 1)*(z_ - 1))/8, -((x_ + 1)*(z_ - 1))/8, -((x_ + 1)*(y_ + 1))/8],\n",
    "                    [ ((y_ + 1)*(z_ + 1))/8,  ((x_ + 1)*(z_ + 1))/8,  ((x_ + 1)*(y_ + 1))/8],\n",
    "                    [-((y_ + 1)*(z_ + 1))/8, -((x_ - 1)*(z_ + 1))/8, -((x_ - 1)*(y_ + 1))/8]]).double()\n",
    "        \n",
    "        # Convert to physical gradient\n",
    "        B_physical = torch.matmul( B , Jinv ).double()\n",
    "        dUx = torch.einsum( 'ijkl,iq->qjkl' , U_N[:,0,:,:,:] , B_physical )\n",
    "        dUy = torch.einsum( 'ijkl,iq->qjkl' , U_N[:,1,:,:,:] , B_physical )\n",
    "        dUz = torch.einsum( 'ijkl,iq->qjkl' , U_N[:,2,:,:,:] , B_physical )\n",
    "        dU = torch.cat( (dUx,dUy,dUz) , dim=0 )\n",
    "\n",
    "        # Strain [ 11 , 22 , 33 , 23 , 13 , 12 ]\n",
    "        eps = torch.einsum( 'qi,ijkl->qjkl' , grad2strain , dU )\n",
    "\n",
    "        # Stress [ 11 , 22 , 33 , 23 , 13 , 12 ]\n",
    "        Cauchy = torch.einsum( 'qi,ijkl->qjkl' , C_elastic , eps )\n",
    "\n",
    "        # Shear stresses need to be counted twice due to symmetry\n",
    "        Cauchy[3:,:,:,:] *= 2.\n",
    "        SE = 0.5 * torch.einsum( 'ijkl,ijkl->jkl' , Cauchy , eps ) \n",
    "\n",
    "        # Scaled by design density\n",
    "        strainEnergy_at_elem += SE * 1. * detJ    \n",
    "    return torch.sum( strainEnergy_at_elem )\n",
    "\n",
    "\n",
    "def CauchyStress(P, F):\n",
    "\n",
    "    detF  = determinant(F)\n",
    "    sigma = torch.pow(detF,-1).view(-1,1,1) * torch.bmm(P,F.permute(0,2,1)) \n",
    "    return sigma\n",
    "\n",
    "def strain(F):\n",
    "\n",
    "    identity = torch.zeros((len(F), 3, 3)); identity[:,0,0]=1; identity[:,1,1]=1; identity[:,2,2]=1\n",
    "    C        = torch.bmm(F.permute(0,2,1),F)\n",
    "    strainCG = 0.5 * (C-identity)\n",
    "    \n",
    "    return strainCG\n",
    "\n",
    "def ConvergenceCheck( arry , rel_tol ):\n",
    "    num_check = 10\n",
    "\n",
    "    if HyperOPT and arry[-1] < -4.:\n",
    "        print('Solution diverged!!!!!!!')\n",
    "        return True\n",
    "\n",
    "    # Run minimum of 2*num_check iterations\n",
    "    if len( arry ) < 2 * num_check :\n",
    "        return False\n",
    "\n",
    "    mean1 = np.mean( arry[ -2*num_check : -num_check ] )\n",
    "    mean2 = np.mean( arry[ -num_check : ] )\n",
    "\n",
    "    if np.abs( mean2 ) < 1e-6:\n",
    "        print('Loss value converged to abs tol of 1e-6' )\n",
    "        return True     \n",
    "\n",
    "    if ( np.abs( mean1 - mean2 ) / np.abs( mean2 ) ) < rel_tol:\n",
    "        print('Loss value converged to rel tol of ' + str(rel_tol) )\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "class DeepMixedMethod:\n",
    "    # Instance attributes\n",
    "    def __init__(self, model):\n",
    "        self.S_Net   = S_Net(model[0], model[1], model[2] , model[4] )\n",
    "        self.S_Net   = self.S_Net.to(dev)\n",
    "        '''The .to() method is used to move a model or tensor to the device specified by dev. \n",
    "        This device can be either a CPU or GPU. The dev variable is typically set to torch.device('cuda') \n",
    "        for GPU or torch.device('cpu') for CPU.'''\n",
    "        numIntType   = 'AD'# 'AD'  'trapezoidal'\n",
    "        self.intLoss = IntegrationLoss(numIntType, 3)                #intLoss is an attribute of object type in here\n",
    "        self.lr = model[3]\n",
    "\n",
    "\n",
    "    def train_model(self, domain):\n",
    "        N_para = 0\n",
    "        for parameter in self.S_Net.parameters():\n",
    "            N_para += np.sum( list(parameter.shape) )\n",
    "        # print( N_para )\n",
    "        # exit()\n",
    "        '''This line iterates over each parameter\n",
    "          in the model self.S_Net. In PyTorch, self.S_Net.parameters() returns an iterator over\n",
    "            all the parameters (weights and biases) of the model.'''\n",
    "                \n",
    "        integrationIE = self.intLoss.lossInternalEnergy\n",
    "        integrationEE = self.intLoss.lossExternalEnergy\n",
    "        nodes   = domain['Domain'].to(dev); nodes.requires_grad_(True);   nodes.retain_grad()\n",
    "        nodesEn = domain['Energy'].to(dev); nodesEn.requires_grad_(True); nodesEn.retain_grad()\n",
    "        X1_indx = domain['X1'].to(dev); X2_indx = domain['X2'].to(dev)\n",
    "        Y1_indx = domain['Y1'].to(dev); Y2_indx = domain['Y2'].to(dev)\n",
    "        Z1_indx = domain['Z1'].to(dev); Z2_indx = domain['Z2'].to(dev)\n",
    "        node_int_indx = domain['Xint'].to(dev)\n",
    "        node_X1_Uni_indx = domain['X1_Uni'].to(dev)\n",
    "        \n",
    "        X1    = nodes[X1_indx,:]; X1.requires_grad_(True); X1.retain_grad()\n",
    "        X2    = nodes[X2_indx,:]; X2.requires_grad_(True); X2.retain_grad()\n",
    "        Y1    = nodes[Y1_indx,:]; Y1.requires_grad_(True); Y1.retain_grad()\n",
    "        Y2    = nodes[Y2_indx,:]; Y2.requires_grad_(True); Y2.retain_grad()\n",
    "        Z1    = nodes[Z1_indx,:]; Z1.requires_grad_(True); Z1.retain_grad()\n",
    "        Z2    = nodes[Z2_indx,:]; Z2.requires_grad_(True); Z2.retain_grad()\n",
    "        nodes_int = nodes[node_int_indx,:]; nodes_int.requires_grad_(True); nodes_int.retain_grad()\n",
    "        X1_Uni = nodes[node_X1_Uni_indx,:]; X1_Uni.requires_grad_(True); X1_Uni.retain_grad()\n",
    "        \n",
    "        identity = torch.zeros((len(nodes), 3, 3)); identity[:,0,0]=1; identity[:,1,1]=1; identity[:,2,2]=1\n",
    "        torch.set_printoptions(precision=8)\n",
    "        self.S_Net.reset_parameters()\n",
    "\n",
    "        \n",
    "        LBFGS_max_iter  = 200\n",
    "        optimizerL = torch.optim.LBFGS(self.S_Net.parameters(), lr=self.lr, max_iter=LBFGS_max_iter, line_search_fn='strong_wolfe', tolerance_change=1e-8, tolerance_grad=1e-8)\n",
    "        LBFGS_loss = {}\n",
    "\n",
    "        disp_history     = np.zeros((step_max+1,len(nodes), 3))\n",
    "        F_histroy        = np.zeros((step_max+1,len(nodes), 3, 3))\n",
    "        strainCG_histroy = np.zeros((step_max+1,len(nodes), 3, 3))\n",
    "        stressPK_histroy = np.zeros((step_max+1,len(nodes), 3, 3))\n",
    "        stressC_histroy = np.zeros((step_max+1,len(nodes), 3, 3))\n",
    "        electric_potential_M2 = torch.zeros( len(nodes) )\n",
    "        E_field = torch.zeros((len(nodes), 3))\n",
    "\n",
    "        for step in range(1,step_max+1):\n",
    "            self.applied_trac = step/step_max * total_traction\n",
    "            # print(self.applied_trac)\n",
    "                \n",
    "            tempL = []\n",
    "            for epoch in range(LBFGS_Iteration):\n",
    "                def closure():\n",
    "                    loss = self.loss_function(step,epoch,nodes,nodes_int,X1_Uni,X1,X2,Y1,Y2,Z1,Z2,nodesEn,self.applied_trac, integrationIE, integrationEE)\n",
    "                    optimizerL.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    tempL.append(loss.item())\n",
    "                    return loss\n",
    "                optimizerL.step(closure)\n",
    "\n",
    "                # Check convergence\n",
    "                if ConvergenceCheck( tempL , rel_tol ):\n",
    "                    break\n",
    "\n",
    "            LBFGS_loss[step] = tempL\n",
    "\n",
    "            u_pred = self.getUP(nodes)          \n",
    "            F_M2        = deformation_gradient(u_pred, nodes)\n",
    "\n",
    "            if Example == 'Hyperelastic':        \n",
    "                F_M2        = deformation_gradient(u_pred, nodes)\n",
    "                strainCG_M2 = strain(F_M2)\n",
    "                stressPK_M2 = stressNH(F_M2)\n",
    "                stressC_M2 = CauchyStress(stressPK_M2, F_M2)\n",
    "\n",
    "            elif Example == 'Elastic':\n",
    "                grad_u    = displacement_gradient(u_pred, nodes)\n",
    "                strainCG_M2 = 0.5 * ( grad_u + grad_u.permute(0,2,1) )\n",
    "                stressC_M2 = stressLE( strainCG_M2 )\n",
    "\n",
    "            elif Example == 'Piezoelectric':\n",
    "                grad_u    = displacement_gradient(u_pred, nodes)\n",
    "                E_field    = electric_potential_gradient(u_pred, nodes)\n",
    "\n",
    "                strainCG_M2 = 0.5 * ( grad_u + grad_u.permute(0,2,1) )\n",
    "\n",
    "                stressC_M2 = stressLE( strainCG_M2 ) - torch.einsum( 'jkl,il->ijk' , e , E_field )\n",
    "\n",
    "                electric_potential_M2 = u_pred[:,3]\n",
    "\n",
    "\n",
    "\n",
    "            disp_history[step,:, :]        = u_pred[:,:3].detach().cpu().numpy()\n",
    "            strainCG_histroy[step,:, :, :] = strainCG_M2.detach().cpu().numpy()\n",
    "            stressC_histroy[step,:, :, :]  = stressC_M2.detach().cpu().numpy()\n",
    "                \n",
    "        return disp_history, F_histroy, strainCG_histroy, stressPK_histroy, stressC_histroy, stressC_M2, strainCG_M2 , electric_potential_M2 , E_field , LBFGS_loss\n",
    "\n",
    "    def getUP(self, nodes):\n",
    "        uP  = self.S_Net.forward(nodes).double()\n",
    "        Ux = nodes[:, 0] * uP[:, 0]\n",
    "        Uy = nodes[:, 0] * uP[:, 1]\n",
    "        Uz = nodes[:, 0] * uP[:, 2]\n",
    "        Ux = Ux.reshape(Ux.shape[0], 1)\n",
    "        Uy = Uy.reshape(Uy.shape[0], 1)\n",
    "        Uz = Uz.reshape(Uz.shape[0], 1)\n",
    "        u_pred = torch.cat((Ux, Uy, Uz), -1)\n",
    "        return u_pred\n",
    "\n",
    "\n",
    "    def loss_function(self,step,epoch,nodes,nodes_int,X1_Uni,X1,X2,Y1,Y2,Z1,Z2,nodesEn,traction,integrationIE, integrationEE):\n",
    "        u_nodesE = self.getUP(nodesEn)\n",
    "        if Example == 'Hyperelastic':\n",
    "            if INT_TYPE == 'AD':\n",
    "                internal = psi(u_nodesE, nodesEn, integrationIE, dx, dy, dz, shape)\n",
    "            else:\n",
    "                internal = psi_Gauss(u_nodesE, nodesEn, integrationIE, dx, dy, dz, shape)\n",
    "\n",
    "        elif Example == 'Elastic':\n",
    "            if INT_TYPE == 'AD':\n",
    "                internal = LE(u_nodesE, nodesEn, integrationIE, dx, dy, dz, shape)\n",
    "            else:\n",
    "                internal = LE_Gauss(u_nodesE, nodesEn, integrationIE, dx, dy, dz, shape)\n",
    "\n",
    "\n",
    "        neu_uP_pred = self.getUP(X1_Uni)[:,:3]\n",
    "        neu_u_pred = neu_uP_pred[:,1]\n",
    "        fext = traction * neu_u_pred \n",
    "        external = integrationEE(fext, dx=dy, dy=dz, shape=[shape[1], shape[2]])\n",
    "        L_E = internal - external\n",
    "        loss =  L_E\n",
    "        print('Step = '+ str(step) + ', Epoch = ' + str( epoch) + ', L = ' + str( loss.item() ) )        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "global Example\n",
    "Example = 'Hyperelastic'\n",
    "# Example = 'Elastic'\n",
    "\n",
    "#INT_TYPE = 'AD'\n",
    "INT_TYPE = 'SF'\n",
    "\n",
    "\n",
    "print('Example = ' + Example + ', using ' + INT_TYPE )\n",
    "base  = './DEM/' + Example + '_' + INT_TYPE + '/'\n",
    "if not os.path.exists( base ):\n",
    "    os.mkdir( base )\n",
    "'''For a fair and consistent comparison, the NN in GCN-DEM and the MLP model in MLP-DEM share the same network structure: they have six\n",
    "layers (including input and output). The number of neurons in each layer is 3, 16, 32, 64, 32, 16, and 3, respectively. The\n",
    "hyperbolic tangent function was used as activation function for all layers except the output, which has linear activation.\n",
    "The L-BFGS optimizer with a fixed learning rate of 0.01 is used to train the models. Training process is stopped when\n",
    "a maximum of 20 training iterations is reached, or when the relative change in loss function value is less than 5 × 10−5.\n",
    "To test the robustness of both methods, we applied the full magnitude of the external load in a single load step, which\n",
    "is in contrast to FEM, where large loads are applied gradually throughout several load steps for better convergence'''\n",
    "\n",
    "# ------------------------------ network settings ---------------------------------------------------\n",
    "D_in  = 3                       #input layer\n",
    "H     = 16                      #hidden layer\n",
    "D_out = 3                       #output layer\n",
    "\n",
    "# ----------------------------- define structural parameters ----------------------------------------\n",
    "Length = 4.0           #cantilever beam with length, width and height\n",
    "Width  = 1.0\n",
    "Depth  = 1.0\n",
    "\n",
    "numb_nodes_cont_param = 10\n",
    "Ny = numb_nodes_cont_param\n",
    "Nz = numb_nodes_cont_param\n",
    "Nx = int((numb_nodes_cont_param-1) * int(Length/Width) + 1)\n",
    "\n",
    "# Nx = 67; Ny = 18; Nz = 18\n",
    "# Nx = 37; Ny = 10; Nz = 10 found during code run\n",
    "\n",
    "\n",
    "shape = [Nx, Ny, Nz]     #[37,10,10]-a list\n",
    "x_min, y_min, z_min = (0.0, 0.0, 0.0)\n",
    "(dx, dy, dz) = (Length / (Nx - 1), Width / (Ny - 1), Depth / (Nz - 1))\n",
    "\n",
    "domain = setup_domain()                                                 #domain is a dictionary\n",
    "print('# of nodes is ', len(domain['Domain']))\n",
    "print('# of interior nodes is ', len(domain['Xint']))\n",
    "print('# of surfaace nodes is ', len(domain['Domain']) - len(domain['Xint']))\n",
    "\n",
    "\n",
    "YM =  1000                      #Young's Modulus\n",
    "PR =  0.3                       #Poisson's Ratio\n",
    "\n",
    "\n",
    "\n",
    "# Loading\n",
    "total_traction = -25.\n",
    "step_max   = 20\n",
    "ref_file = './AbaqusReferenceDisplacements/' + 'NH_Disp25_'\n",
    "\n",
    "\n",
    "# Training\n",
    "LBFGS_Iteration = 20\n",
    "rel_tol = 5e-5\n",
    "\n",
    "\n",
    "\n",
    "# Initial hyper parameters\n",
    "x_var = { 'x_lr' : 0.01 ,\n",
    "         'neuron' : 16 ,\n",
    "         'act_func' : 'tanh' }\n",
    "\n",
    "def Obj( x_var ):\n",
    "    lr = x_var['x_lr']\n",
    "    H = int(x_var['neuron'])\n",
    "    act_fn = x_var['act_func']\n",
    "    print( 'LR: ' + str(lr) + ', H: ' + str(H) + ', act fn: ' + act_fn )\n",
    "\n",
    "\n",
    "    dcm = DeepMixedMethod([D_in, H, D_out, lr , act_fn])           #dcm is an object of class DeepMixedMethod in here\n",
    "    start_time = time.time()\n",
    "    disp_history, F_histroy, strainCG_histroy, stressPK_histroy, stressC_history, stressC_last, strain_last , electric_potential_last , E_field , LBFGS_loss = dcm.train_model(domain)\n",
    "    '''10 outputs are given by train_model function'''\n",
    "    end_time = time.time()\n",
    "    print('simulation time = ' + str(end_time - start_time) + 's')\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################################\n",
    "    # Save data\n",
    "    x_space = np.expand_dims(domain['Domain'][:,0].detach().cpu().numpy(), axis=1)\n",
    "    y_space = np.expand_dims(domain['Domain'][:,1].detach().cpu().numpy(), axis=1)\n",
    "    z_space = np.expand_dims(domain['Domain'][:,2].detach().cpu().numpy(), axis=1)\n",
    "    coordin = np.concatenate((x_space, y_space, z_space), axis=1)\n",
    "    U = disp_history[-1,:,:]\n",
    "\n",
    "    Nodal_Strain = torch.cat((strain_last[:,0,0].unsqueeze(1),strain_last[:,1,1].unsqueeze(1),strain_last[:,2,2].unsqueeze(1),\\\n",
    "                              strain_last[:,0,1].unsqueeze(1),strain_last[:,1,2].unsqueeze(1),strain_last[:,0,2].unsqueeze(1)),axis=1)\n",
    "    Nodal_Stress = torch.cat((stressC_last[:,0,0].unsqueeze(1),stressC_last[:,1,1].unsqueeze(1),stressC_last[:,2,2].unsqueeze(1),\\\n",
    "                              stressC_last[:,0,1].unsqueeze(1),stressC_last[:,1,2].unsqueeze(1),stressC_last[:,0,2].unsqueeze(1)),axis=1)\n",
    "    Nodal_E = torch.cat((E_field[:,0].unsqueeze(1),E_field[:,1].unsqueeze(1),E_field[:,2].unsqueeze(1)),axis=1)\n",
    "\n",
    "    stress_vMis = torch.pow(0.5 * (torch.pow((Nodal_Stress[:,0]-Nodal_Stress[:,1]), 2) + torch.pow((Nodal_Stress[:,1]-Nodal_Stress[:,2]), 2)\n",
    "                   + torch.pow((Nodal_Stress[:,2]-Nodal_Stress[:,0]), 2) + 6 * (torch.pow(Nodal_Stress[:,3], 2) +\n",
    "                     torch.pow(Nodal_Stress[:,4], 2) + torch.pow(Nodal_Stress[:,5], 2))), 0.5)\n",
    "    Nodal_Strain = Nodal_Strain.cpu().detach().numpy()\n",
    "    Nodal_Stress = Nodal_Stress.cpu().detach().numpy()\n",
    "    stress_vMis  = stress_vMis.unsqueeze(1).cpu().detach().numpy()\n",
    "    electric_potential = electric_potential_last.unsqueeze(1).cpu().detach().numpy()\n",
    "    Nodal_E = Nodal_E.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Data = np.concatenate((coordin, U, Nodal_Strain , Nodal_Stress, stress_vMis, electric_potential , Nodal_E), axis=1)\n",
    "    np.save( base + 'Results.npy',Data)\n",
    "\n",
    "    LBFGS_loss_D1 = np.array(LBFGS_loss[1])\n",
    "    fn = base + 'Training_loss.npy'\n",
    "    np.save( fn , LBFGS_loss_D1 )\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################################\n",
    "    # Write vtk\n",
    "    def FormatMe( v ):\n",
    "        S = [Nz,Nx,Ny]\n",
    "        return np.swapaxes( np.swapaxes( v.reshape(S) , 0 , 1 ) , 1 , 2 ).flatten('F')\n",
    "\n",
    "    grid = pv.UniformGrid()\n",
    "    grid.dimensions = np.array([Nx,Ny,Nz])\n",
    "    grid.origin = np.zeros(3)\n",
    "    grid.spacing = np.array([dx,dy,dz])\n",
    "    names = [ 'Ux' , 'Uy' , 'Uz' , 'E11' , 'E22' , 'E33' , 'E12' , 'E23' , 'E13' , 'S11' , 'S22' , 'S33' , 'S12' , 'S23' , 'S13' , 'SvM' , 'E_pot' , 'D1' , 'D2' , 'D3']\n",
    "    for idx , n in enumerate( names ):\n",
    "        grid.point_data[ n ] =  FormatMe( Data[:,idx+3] )\n",
    "\n",
    "\n",
    "\n",
    "    #############################################################################################\n",
    "    # Abaqus comparison\n",
    "    Out = np.load( ref_file + '.npy' )\n",
    "\n",
    "    names = [ 'Ux_ABQ' , 'Uy_ABQ' , 'Uz_ABQ' ]\n",
    "    for idx , n in enumerate( names ):\n",
    "        grid.point_data[ n ] =  Out[idx].flatten('F')\n",
    "\n",
    "    # Compute difference\n",
    "    names = [ 'Ux' , 'Uy' , 'Uz' ]\n",
    "    diff = []\n",
    "    for idx , n in enumerate( names ):\n",
    "        FEM = grid.point_data[ n + '_ABQ' ]\n",
    "        ML = grid.point_data[ n ]\n",
    "        grid.point_data[ n + '_diff' ] =  np.abs( FEM - ML ) / np.max( np.abs(FEM) ) * 100.\n",
    "        diff.append( np.mean(grid.point_data[ n + '_diff' ]) )\n",
    "    grid.save( base + \"Results.vti\")\n",
    "\n",
    "    mE = np.mean(diff)\n",
    "    print( 'Mean error in U compared to Abaqus: ' + str(mE) )\n",
    "    return np.mean(diff)\n",
    "\n",
    "Obj( x_var )                #First function to be called\n",
    "\n",
    "#implement this code for adhesive contact "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
